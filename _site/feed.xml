<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-23T01:10:48+08:00</updated><id>http://localhost:4000/</id><entry><title type="html">策略梯度一个公式的推导</title><link href="http://localhost:4000/2020/04/23/policy-gradient/" rel="alternate" type="text/html" title="策略梯度一个公式的推导" /><published>2020-04-23T00:00:00+08:00</published><updated>2020-04-23T00:00:00+08:00</updated><id>http://localhost:4000/2020/04/23/policy-gradient</id><content type="html" xml:base="http://localhost:4000/2020/04/23/policy-gradient/">&lt;h1 id=&quot;策略梯度一个公式的推导&quot;&gt;策略梯度一个公式的推导&lt;/h1&gt;

&lt;h3 id=&quot;所有轨迹下t-时刻的-reward-r_t-的期望-e_taur_t-的策略梯度&quot;&gt;所有轨迹下，\(t\) 时刻的 reward \(r_t\) 的期望 \(E_{\tau}r_t\) 的策略梯度&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}E_{\tau}r_t=\nabla_{\theta}{\sum_{\tau}p_{\theta}(\tau)r_t}= \\
\sum_{\tau}\nabla_{\theta}p_{\theta}(\tau)r_t=\sum_{\tau}{p_{\theta}(\tau)\nabla_{\theta}log\{p_{\theta}(\tau)\}r_t}=\\
\sum_{\tau}p_{\theta}(\tau)\{\sum_{i=0}^{T-1}\nabla_{\theta}log\{\pi_{\theta}(a_i|t_i)\} \}r_t&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;对中间的梯度之和只取第-tk-项-k0&quot;&gt;对中间的梯度之和，只取第 \(t+k\) 项, \(k&amp;gt;0\)&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{\tau}{p_{\theta}(\tau)\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}r_t}=\\
\sum_{s_0,a_0..s_{T-1},a_{T-1}}p(s_0)\cdot \prod_{i=0}^{T-1}\pi_{\theta}(a_i|s_i)\cdot p(s_{i+1}|s_i,a_i)\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}r_t=\\
\sum_{s_0,a_0..s_{t+k},a_{t+k}}\sum_{s_{t+k+1},a_{t+k+1}..s_{T-1},a_{T-1}}p(s_0)\cdot \prod_{i=0}^{T-1}\pi_{\theta}(a_i|s_i)\cdot p(s_{i+1}|s_i,a_i)\cdot \\
\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}r_t=\\
\sum_{s_0,a_0..s_{t+k},a_{t+k}}p(s_0)\cdot \prod_{i=0}^{t+k-1}\pi_{\theta}(a_i|s_i)\cdot p(s_{i+1}|s_i,a_i)\cdot \pi_{\theta}(a_{t+k}|s_{t+k})\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}r_t \cdot \\
\sum_{s_{t+k+1},a_{t+k+1}..s_{T-1},a_{T-1}} p(s_{i+1}|s_i,a_i) \cdot\prod_{i=t+k+1}^{T-1}\pi_{\theta}(a_i|s_i)\cdot p(s_{i+1}|s_i,a_i)&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;考察&quot;&gt;考察&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{s_0,a_0..s_{t+k},a_{t+k}}p(s_0) \prod_{i=0}^{t+k-1}\pi_{\theta}(a_i|s_i)\cdot p(s_{i+1}|s_i,a_i)\cdot \pi_{\theta}(a_{t+k}|s_{t+k})\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}r_t=\\
\sum_{s_0,a_0..s_{t+k}}r_t \cdot p(s_0) \prod_{i=0}^{t+k-1}\pi_{\theta}(a_i|s_i)\cdot p(s_{i+1}|s_i,a_i)\cdot \sum_{a_{t+k}}\pi_{\theta}(a_{t+k}|s_{t+k})\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;给定s_tk-其中&quot;&gt;给定\(s_{t+k}\), 其中&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{a_{t+k}}\pi_{\theta}(a_{t+k}|s_{t+k})\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}=\\
\sum_{a_{t+k}}\pi_{\theta}(a_{t+k}|s_{t+k})\frac{\nabla_{\theta}\pi_{\theta}(a_{t+k}|s_{t+k})}{\pi_{\theta}(a_{t+k}|s_{t+k})}=\sum_{a_{t+k}}\nabla_{\theta}\pi_{\theta}(a_{t+k}|s_{t+k})=\\
\nabla_{\theta}\sum_{a_{t+k}}\pi_{\theta}(a_{t+k}|s_{t+k})=\nabla_{\theta}1=0&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;从而可知当-k0&quot;&gt;从而可知，当 \(k&amp;gt;0\)&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{\tau}{p_{\theta}(\tau)\nabla_{\theta}log\{\pi_{\theta}(a_{t+k}|s_{t+k})\}r_t}=0\Longrightarrow \\
\sum_{\tau}p_{\theta}(\tau)\{\sum_{i=0}^{T-1}\nabla_{\theta}log\{\pi_{\theta}(a_i|t_i)\} \}r_t=\sum_{\tau}p_{\theta}(\tau)\{\sum_{i=0}^{t}\nabla_{\theta}log\{\pi_{\theta}(a_i|t_i)\} \}r_t \Longrightarrow \\
\nabla_{\theta}E_{\tau}r_t=\sum_{\tau}p_{\theta}(\tau)\{\sum_{i=0}^{t}\nabla_{\theta}log\{\pi_{\theta}(a_i|t_i)\} \}r_t&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;即-t-时刻的奖励的期望的策略梯度与-t-时刻之后的各时刻的策略梯度无关&quot;&gt;即 \(t\) 时刻的奖励的期望的策略梯度与 \(t\) 时刻之后的各时刻的策略梯度无关。&lt;/h3&gt;</content><author><name></name></author><summary type="html">策略梯度一个公式的推导</summary></entry><entry><title type="html">gumbel 与 softmax</title><link href="http://localhost:4000/2020/04/09/gumbel/" rel="alternate" type="text/html" title="gumbel 与 softmax" /><published>2020-04-09T00:00:00+08:00</published><updated>2020-04-09T00:00:00+08:00</updated><id>http://localhost:4000/2020/04/09/gumbel</id><content type="html" xml:base="http://localhost:4000/2020/04/09/gumbel/">&lt;h1 id=&quot;gumbel-与-softmax&quot;&gt;gumbel 与 softmax&lt;/h1&gt;

&lt;h3 id=&quot;对于变量-x_1x_2x_k以及标准-gumbel-分布上的噪声-epsilon_1epsilon_2epsilon_k以下操作&quot;&gt;对于变量 \(x_1,x_2,..x_k\)，以及&lt;a href=&quot;https://en.wikipedia.org/wiki/Gumbel_distribution&quot;&gt;标准 gumbel 分布&lt;/a&gt;上的噪声 \(\epsilon_1,\epsilon_2,..\epsilon_k\)，以下操作：&lt;/h3&gt;

&lt;p&gt;\[argmax(x_1+\epsilon_1, x_2+\epsilon_2,..x_k+\epsilon_k)\]&lt;/p&gt;

&lt;h3 id=&quot;等同于在-softmaxx_1x_2x_k-这个多元离散分布上采样&quot;&gt;等同于在 \(softmax(x_1,x_2,..x_k)\) 这个多元离散分布上采样。&lt;/h3&gt;

&lt;h2 id=&quot;证明&quot;&gt;证明：&lt;/h2&gt;

&lt;h3 id=&quot;在-softmaxx_1x_2x_k-采样得到-i-的概率是&quot;&gt;在 \(softmax(x_1,x_2,..x_k)\) 采样得到 \(i\) 的概率是：&lt;/h3&gt;
&lt;p&gt;\[ \frac{e^{x_i}}{\sum_{j=1}^{k}e^{x_j}} \tag{*} \]&lt;/p&gt;

&lt;h3 id=&quot;另一方面-argmaxx_1epsilon_1-x_2epsilon_2x_kepsilon_ki-的意思是说&quot;&gt;另一方面 \(argmax(x_1+\epsilon_1, x_2+\epsilon_2,..x_k+\epsilon_k)=i\) 的意思是说&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
x_1+\epsilon_1&lt;x_i+\epsilon_i \\
x_2+\epsilon_2&lt;x_i+\epsilon_i \\
...\\
x_k+\epsilon_k&lt;x_i+\epsilon_i \\ %]]&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h3 id=&quot;等价于&quot;&gt;等价于&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\epsilon_1&lt;x_i+\epsilon_i-x_1 \\
\epsilon_2&lt;x_i+\epsilon_i-x_2 \\
...\\
\epsilon_k&lt;x_i+\epsilon_i-x_k \\ %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;给定-epsilon_i-其概率为&quot;&gt;给定 \(\epsilon_i\) ，其概率为：&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\prod_{j\neq{i}}e^{-e^{-(x_i+\epsilon_i-x_j)}} \\
=e^{-\sum_{j\neq{i}}e^{-(x_i+\epsilon_i-x_j)}}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;遍历-epsilon_i-的所有取值&quot;&gt;遍历 \(\epsilon_i\) 的所有取值，&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p\{argmax(x_1+\epsilon_1, x_2+\epsilon_2,..x_k+\epsilon_k)=i\} \\
=\int^{+\infty}_{-\infty} e^{-\epsilon_i-e^{-\epsilon_i}}\cdot e^{-\sum_{j\neq{i}}e^{-(x_i+\epsilon_i-x_j)}} d\epsilon_i&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;指数部分化简为&quot;&gt;指数部分化简为：&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\epsilon_i-e^{-\epsilon_i}\cdot {(1+\sum_{j\neq{i}}\frac{e^{x_j}}{e^{x_i}})} \\
=-\epsilon_i-\frac {e^{-\epsilon_i}} {\frac{e^{x_i}}{\sum_{j=1}^{k} e^{x_j}}}&lt;/script&gt;

&lt;h3 id=&quot;令-fracex_isum_j1k-ex_ja-上面的积分可表示为&quot;&gt;令 \({\frac{e^{x_i}}{\sum_{j=1}^{k} e^{x_j}}}=a\) ，上面的积分可表示为：&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\int^{+\infty}_{-\infty} e^{-\epsilon_i-\frac{e^{-\epsilon_i}}{a}}d\epsilon_i&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;通过-wolfram-网站可得该积分为-a-cdot-efrac1acdot--e-xinfty_-infty--a-从而也就说明-pargmaxx_1epsilon_1-x_2epsilon_2x_kepsilon_kifracex_isum_j1k-ex_j&quot;&gt;通过 wolfram 网站可得该积分为 \(a \cdot e^{\frac{1}{a}\cdot -e^{-x}}|^{+\infty}_{-\infty} = a\) ，从而也就说明 &lt;script type=&quot;math/tex&quot;&gt;p\{argmax(x_1+\epsilon_1, x_2+\epsilon_2,..x_k+\epsilon_k)=i\}={\frac{e^{x_i}}{\sum_{j=1}^{k} e^{x_j}}}&lt;/script&gt;&lt;/h3&gt;

&lt;h3 id=&quot;结合--式命题得证&quot;&gt;结合 (*) 式，命题得证。&lt;/h3&gt;

&lt;h2 id=&quot;作用&quot;&gt;作用：&lt;/h2&gt;

&lt;h3 id=&quot;当需要将多元离散分布上的样本作为神经网络的输入时一种方法是将该采样以-onehot-编码传入神经网络通常这一操作可用带温度-softmax-来模拟&quot;&gt;当需要将多元离散分布上的样本作为神经网络的输入时，一种方法是将该采样以 onehot 编码传入神经网络。通常这一操作可用带温度 \(softmax\) 来模拟：&lt;/h3&gt;
&lt;p&gt;\(softmax(\frac{x_1+\epsilon_1}{T}, \frac{x_2+\epsilon_2}{T},..\frac{x_k+\epsilon_k}{T})\)&lt;/p&gt;

&lt;h3 id=&quot;其中-t-越小近似程度越高&quot;&gt;其中 \(T\) 越小，近似程度越高。&lt;/h3&gt;
&lt;h3 id=&quot;如果-x_1x_2x_k-是一个神经网络的输出那么通过上面的加-gumbel-噪声以及带温度的-softmax-两个操作我们用可导连续计算模拟了不可导的采样动作从而沟通了两个神经网络使得反向梯度传播成为可能类比于-vae-中对高斯分布采样的-reparameter-trick这里是对多元离散分布的-reparameter-trick-&quot;&gt;如果 \(x_1,x_2..x_k\) 是一个神经网络的输出，那么通过上面的加 \(gumbel\) 噪声以及带温度的 \(softmax\) 两个操作，我们用可导连续计算模拟了不可导的采样动作，从而沟通了两个神经网络，使得反向梯度传播成为可能。类比于 VAE 中对高斯分布采样的 reparameter trick，这里是对多元离散分布的 reparameter trick 。&lt;/h3&gt;

&lt;h2 id=&quot;reference&quot;&gt;reference：&lt;/h2&gt;

&lt;p&gt;https://en.wikipedia.org/wiki/Gumbel_distribution&lt;/p&gt;

&lt;p&gt;https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/&lt;/p&gt;

&lt;p&gt;https://arxiv.org/pdf/1611.01144.pdf&lt;/p&gt;</content><author><name></name></author><summary type="html">gumbel 与 softmax</summary></entry><entry><title type="html">simhash 原理</title><link href="http://localhost:4000/2020/04/04/simhash/" rel="alternate" type="text/html" title="simhash 原理" /><published>2020-04-04T00:00:00+08:00</published><updated>2020-04-04T00:00:00+08:00</updated><id>http://localhost:4000/2020/04/04/simhash</id><content type="html" xml:base="http://localhost:4000/2020/04/04/simhash/">&lt;h1 id=&quot;simhash-原理&quot;&gt;simhash 原理&lt;/h1&gt;

&lt;h3 id=&quot;simhash-算法一般用来计算两个文本的相似度&quot;&gt;simhash 算法一般用来计算两个文本的相似度。&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;设想两个文本被表示成两个向量向量中元素的值表示一个特征的权重比如某个词的出现频数&quot;&gt;设想两个文本被表示成两个向量，向量中元素的值表示一个特征的权重，比如某个词的出现频数。&lt;/h3&gt;

&lt;p&gt;\[v_i=(w^{(i)}_1, w^{(i)}_2 .. w^{(i)}_D)\]&lt;/p&gt;

&lt;p&gt;\[v_j=(w^{(j)}_1, w^{(j)}_2 .. w^{(j)}_D)\]&lt;/p&gt;

&lt;h3 id=&quot;对于两个向量如何估计其相似度一种方法是计算余弦相似度实际应用中两两计算余弦相似度的开销可能过高尤其是特征特别多的情况下如果我们能将特征向量转换成较短的签名可能可以提高计算效率&quot;&gt;对于两个向量，如何估计其相似度？一种方法是计算余弦相似度。实际应用中，两两计算余弦相似度的开销可能过高，尤其是特征特别多的情况下。如果我们能将特征向量转换成较短的签名，可能可以提高计算效率。&lt;/h3&gt;

&lt;h3 id=&quot;对于-d-维随机向量-u-v_i-cdot-u-与-v_j-cdot-u-符号相反的概率正比于-v_i-v_j-的夹角大小&quot;&gt;对于 \(D\) 维随机向量 \(u\) ，\(v_i \cdot u\) 与 \(v_j \cdot u\) 符号相反的概率正比于 \(v_i, v_j\) 的夹角大小。&lt;/h3&gt;

&lt;h3 id=&quot;选定-n-个随机向量-u_1n-以下两个向量&quot;&gt;选定 \(n\) 个随机向量 \(u_{1..n}\) ，以下两个向量&lt;/h3&gt;
&lt;p&gt;\[(sgn(v_i \cdot u_1), sgn(v_i \cdot u_2) .. sgn(v_i \cdot u_n))^T\]&lt;/p&gt;

&lt;p&gt;\[(sgn(v_j \cdot u_1), sgn(v_j \cdot u_2) .. sgn(v_j \cdot u_n))^T\]&lt;/p&gt;
&lt;h3 id=&quot;的海明距离正比于-v_i-v_j-的夹角大小&quot;&gt;的海明距离正比于 \(v_i, v_j\) 的夹角大小。&lt;/h3&gt;

&lt;h3 id=&quot;海明距离在计算机中计算效率非常高如果同时适当控制-n-的大小我们可以得到一种更高效估计文本相似度的方法&quot;&gt;海明距离在计算机中计算效率非常高，如果同时适当控制 \(n\) 的大小，我们可以得到一种更高效估计文本相似度的方法。&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;simhash-将每个特征-hash-到一个大整数比如-64-bit得到-d-个大整数&quot;&gt;simhash 将每个特征 hash 到一个大整数（比如 64 bit），得到 \(D\) 个大整数：&lt;/h3&gt;
&lt;p&gt;\[(hash(f_1), hash(f_2)..hash(f_D))\]&lt;/p&gt;

&lt;h3 id=&quot;将每个大整数比特位的-0-换成--1-并表示成向量形式上述向量可写成矩阵&quot;&gt;将每个大整数比特位的 \(0\) 换成 \(-1\) ，并表示成向量形式，上述向量可写成矩阵：&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left( \begin{array}{ccc}
b^{(1)}_1 &amp; b^{(2)}_1 &amp; .. &amp; b^{(D)}_1 \\
b^{(1)}_2 &amp; b^{(2)}_2 &amp; .. &amp; b^{(D)}_2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
b^{(1)}_{64} &amp; b^{(2)}_{64} &amp; .. &amp; b^{(D)}_{64} \end{array} \right) %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;根据-hash-函数的性质其中的每一行可看成一个随机向量&quot;&gt;根据 hash 函数的性质，其中的每一行可看成一个随机向量。&lt;/h3&gt;

&lt;h3 id=&quot;simhash-算法中将文档各特征的权重向量与该矩阵行向量做点乘并取符号得到的-64-位向量作为该文档的-simhash-文档之间的相似度用海明距离来度量&quot;&gt;simhash 算法中将文档各特征的权重向量与该矩阵行向量做点乘并取符号，得到的 64 位向量作为该文档的 simhash ，文档之间的相似度用海明距离来度量。&lt;/h3&gt;

&lt;h3 id=&quot;可以看到这种做法原理上与上面取定随机向量的方法是一致的唯一的区别在于并不是取定-64-个随机向量而是通过对特征做-hash-的方法得到-64-个向量这样得到的向量依然具有随机性质&quot;&gt;可以看到这种做法原理上与上面取定随机向量的方法是一致的，唯一的区别在于并不是取定 64 个随机向量，而是通过对特征做 hash 的方法得到 64 个向量，这样得到的向量依然具有随机性质。&lt;/h3&gt;</content><author><name></name></author><summary type="html">simhash 原理</summary></entry><entry><title type="html">我所理解的 AlphaGo Zero 算法</title><link href="http://localhost:4000/2017/11/22/alphago-zero/" rel="alternate" type="text/html" title="我所理解的 AlphaGo Zero 算法" /><published>2017-11-22T00:00:00+08:00</published><updated>2017-11-22T00:00:00+08:00</updated><id>http://localhost:4000/2017/11/22/alphago-zero</id><content type="html" xml:base="http://localhost:4000/2017/11/22/alphago-zero/">&lt;h1 id=&quot;我所理解的-alphago-zero-算法&quot;&gt;我所理解的 AlphaGo Zero 算法&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;神经网络只有一张，输入是棋局最近16步，双方各自最近8步的历史棋局状态。&lt;/li&gt;
  &lt;li&gt;神经网络用来拟合两个函数，因而有两个输出。其中一个输出是一个离散分布，给出19*19个位置和 pass 共362个动作选项的概率分布。另一个输出是-1和1之间的实数值，表示对当前局面的一个打分，-1表示输，1表示赢。&lt;/li&gt;
  &lt;li&gt;算法主要分为两个部分，一部分是神经网络的训练和评估；另一部分则是基于 MCTS 的自我走子。&lt;/li&gt;
  &lt;li&gt;对于围棋这种完全信息的零和博弈，理论上是可以用搜索算法完美解决的。当然前提是我们有足够的计算资源，而这一点是没法做到的，毕竟搜索的状态空间比全宇宙的原子数还要多。&lt;/li&gt;
  &lt;li&gt;神经网络的作用是在不搜索到状态树根据规则胜负已分的局面的前提下也能给出对当前局面的一个比较合理的评估。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;整个算法表示在下图中：
 &lt;img src=&quot;https://gameofdimension.github.io/images/15113235674954.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;每一个自我走子（self-play）阶段包含25,000局自我对战的棋局，每一局都要战到最后状态。&lt;/li&gt;
  &lt;li&gt;每一步的具体走子选择经由 1,600 次模拟（搜索）决定，每一次模拟都是在当前的搜索树中进行。&lt;/li&gt;
  &lt;li&gt;每一次走子搜索的结果是一个概率分布，给出的是当前状态下搜索过程认为的下子选择，这个走子选择被认为优于神经网络走子选择，因此是神经网络的拟合对象，这个概率分布以及当前的棋局状态会保存下来作为后续神经网络的训练数据。&lt;/li&gt;
  &lt;li&gt;每一局自我走子走到最后，根据围棋规则可以分出胜负，从而被打分-1或者1，也就是上图中的 z 值。这个值会和上面的搜索过程走子概率分布一样附加到每一步的棋局状态上，作为神经网络要拟合的另一个目标。&lt;/li&gt;
  &lt;li&gt;神经网络训练会从积累的训练数据集中抽取数据进行训练，训练完成之后会使用该网络和之前的网络进行对抗，如果新网络胜率超过55%，则将更新当前网络为新网络。并循环执行以上各步骤。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;搜索过程如下：
&lt;img src=&quot;https://gameofdimension.github.io/images/15113272393954.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;搜索树中节点代表棋局状态，边代表走子选择，每一条边上会有几个数值，这几个数值是搜索过程中走子选择的依据。每一次具体的搜索过程也会改变搜索路径上每一条边的这些值。&lt;/li&gt;
  &lt;li&gt;每当搜索树扩展出新节点时，新节点下的走子选择以及新节点的价值会由当前的神经网络给出。得到的新节点的价值会导致路径上各条边的价值都得到更新。每次扩展到新节点也表示本次搜索过程结束。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;参考：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://deepmind.com/documents/119/agz_unformatted_nature.pdf&quot;&gt;Mastering the Game of Go without Human Knowledge - DeepMind&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0&quot;&gt;AlphaGo Zero Explained In One Diagram&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">我所理解的 AlphaGo Zero 算法</summary></entry><entry><title type="html">明天太阳还会照常升起吗？</title><link href="http://localhost:4000/2017/11/03/bayesian-inference/" rel="alternate" type="text/html" title="明天太阳还会照常升起吗？" /><published>2017-11-03T00:00:00+08:00</published><updated>2017-11-03T00:00:00+08:00</updated><id>http://localhost:4000/2017/11/03/bayesian-inference</id><content type="html" xml:base="http://localhost:4000/2017/11/03/bayesian-inference/">&lt;h1 id=&quot;明天太阳还会照常升起吗&quot;&gt;明天太阳还会照常升起吗？&lt;/h1&gt;

&lt;h3 id=&quot;罗辑思维讲过贝叶斯思维即所谓不断用新的证据还更新自己的原有认知作为理科生要让文科生来科普贝叶斯自然羞耻但同样羞耻的是我居然不知道这样的更新过程在数学上到底是如何实现的&quot;&gt;罗辑思维讲过贝叶斯思维，即所谓不断用新的证据还更新自己的原有认知。作为理科生要让文科生来科普贝叶斯自然羞耻，但同样羞耻的是我居然不知道这样的更新过程在数学上到底是如何实现的。&lt;/h3&gt;

&lt;h3 id=&quot;比如今天太阳照样升起了跟昨天以及我有生以来的任何一天一样那相比于今天我对于明天太阳升起的信念该如何变化呢&quot;&gt;比如今天太阳照样升起了——跟昨天以及我有生以来的任何一天一样，那相比于今天，我对于明天太阳升起的信念该如何变化呢。&lt;/h3&gt;

&lt;h3 id=&quot;假设今天只是我来到人世间的第三天而已在今天太阳升起之前考虑到昨天前天太阳已经升起过了我假定今天太阳升起的概率是051略微高于瞎蒙的05套用贝叶斯的那一套这是否就是所谓的先验呢现在已是中午今天的太阳已经升起过了那明天太阳升起的概率该是多少这是否就是所谓的后验套用贝叶斯公式来进行计算&quot;&gt;假设今天只是我来到人世间的第三天而已。在今天太阳升起之前，考虑到昨天前天太阳已经升起过了，我假定今天太阳升起的概率是0.51，略微高于瞎蒙的0.5，套用贝叶斯的那一套，这是否就是所谓的先验呢？现在已是中午，今天的太阳已经升起过了，那明天太阳升起的概率该是多少，这是否就是所谓的后验？套用贝叶斯公式来进行计算：&lt;/h3&gt;

&lt;p&gt;\[ p(X_3=1|X_1=1,X_2=1) = \frac {p(X_3=1)\cdotp(X_1=1,X_2=1|X_3=1)} {p(X_1=1,X_2=1)} \]&lt;/p&gt;

&lt;h3 id=&quot;到此我们遇到了困难因为我们不知道怎么计算-px_11x_21x_31-和-px_11x_21昨天和前天太阳都已经升起了px_11x_21应该等于1吗答案是不能我们假定如果是在未来的连续三天上做同样的分析那显然没法说这个概率就是1了另外我们能说-x_1-x_2-x_3独立吗如果是的话上面的推导都没有意义了因为这样的话-px_31x_11x_21--px_31也就是后验等于先验我们的信心没有改变&quot;&gt;到此我们遇到了困难，因为我们不知道怎么计算 \(p(X_1=1,X_2=1|X_3=1)\) 和 \(p(X_1=1,X_2=1)\)。昨天和前天太阳都已经升起了，\(p(X_1=1,X_2=1)\)应该等于1吗？答案是不能，我们假定如果是在未来的连续三天上做同样的分析，那显然没法说这个概率就是1了。另外我们能说 \(X_1, X_2, X_3\)独立吗，如果是的话，上面的推导都没有意义了，因为这样的话\( p(X_3=1|X_1=1,X_2=1) = p(X_3=1)\)，也就是后验等于先验，我们的信心没有改变。&lt;/h3&gt;

&lt;h3 id=&quot;老实说这个疑问徘徊心头至少也有几个月了直到我来到这个知乎页面贝叶斯推断如何更新后验概率坦率讲这里面很多人的答案我都没有看懂但是当我看到这一句用贝叶斯的写法先验概率是一个分布突然有了被击中的感觉于是我大概知道了上面太阳升起的例子该如何解释了&quot;&gt;老实说这个疑问徘徊心头至少也有几个月了，直到我来到这个知乎页面&lt;a href=&quot;https://www.zhihu.com/question/27398304&quot;&gt;“贝叶斯推断如何更新后验概率？”&lt;/a&gt;。坦率讲这里面很多人的答案我都没有看懂，但是当我看到这一句&lt;em&gt;“用贝叶斯的写法，先验概率是一个分布。”&lt;/em&gt;突然有了被击中的感觉。于是我大概知道了上面太阳升起的例子该如何解释了。&lt;/h3&gt;

&lt;h3 id=&quot;首先我们的先验应该是针对某个重复发生的现象或者说是针对某个理论而不是跟上面一样针对某次具体实验因为具体实验发生了就发生了再也没有同一个了也就不存在更新信念的问题其实我们想知道的是关于太阳升起的概率可以对照成某枚硬币抛出head的概率的理论&quot;&gt;首先我们的先验应该是针对某个重复发生的现象，或者说是针对某个理论，而不是跟上面一样针对某次具体实验，因为具体实验发生了就发生了，再也没有同一个了，也就不存在更新信念的问题。其实我们想知道的是关于太阳升起的概率（可以对照成某枚硬币抛出head的概率）的理论。&lt;/h3&gt;

&lt;h3 id=&quot;然后我们应用上面提到的先验概率是个分布的想法这里又需要一个假设即这个先验分布该是怎样的我们假定关于太阳升起的概率这个值本身符合以下分布&quot;&gt;然后我们应用上面提到的先验概率是个分布的想法。这里又需要一个假设，即这个先验分布该是怎样的。我们假定关于太阳升起的概率这个值本身符合以下分布：&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;q&lt;/th&gt;
      &lt;th&gt;0.5&lt;/th&gt;
      &lt;th&gt;0.6&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;也就是说我认为太阳升起的概率只有两种可能或者为05或者为06同时我对这个值为05的信心为09是06的信心只有01&quot;&gt;也就是说我认为太阳升起的概率只有两种可能，或者为0.5，或者为0.6，同时我对这个值为0.5的信心为0.9，是0.6的信心只有0.1。&lt;/h3&gt;

&lt;h3 id=&quot;有了以上的铺垫我们再来分析假定观察到连续两天太阳升起了太阳升起的概率这个值的分布该如何变化呢&quot;&gt;有了以上的铺垫，我们再来分析。假定观察到连续两天太阳升起了，太阳升起的概率这个值的分布该如何变化呢：&lt;/h3&gt;
&lt;p&gt;\[ p(q=0.5|X_1=1,X_2=1) = \frac {p(q=0.5)\cdotp(X_1=1,X_2=1|q=0.5)} {p(X_1=1,X_2=1)} = \frac {0.9\cdot0.5\cdot0.5} {p(X_1=1,X_2=1)}\]&lt;/p&gt;

&lt;p&gt;\[ p(q=0.6|X_1=1,X_2=1) = \frac {p(q=0.6)\cdotp(X_1=1,X_2=1|q=0.6)} {p(X_1=1,X_2=1)} = \frac {0.1\cdot0.6\cdot0.6} {p(X_1=1,X_2=1)}\]&lt;/p&gt;

&lt;h3 id=&quot;二者的比值--frac-09cdot05cdot05-01cdot06cdot06-lt-frac-09-01--也就是说后验上我们对06这个值的信心更高了而且随着连续观察的次数n增加我们的信息会越来越大直到最后我们几乎确定太阳升起的概率不是05而是更高的06这里对于q分布假设只是个简单的两点分布而如果我们假设q的分布更复杂一点并且其最大可能值099分配到了一个很小的非零先验概率000001类似上面的分析虽然先验很低但是不要多少天之后屌丝会逆袭届时我们将笃定太阳明天升起的概率是099&quot;&gt;二者的比值 \( \frac {0.9\cdot0.5\cdot0.5} {0.1\cdot0.6\cdot0.6} \lt \frac {0.9} {0.1} \) 也就是说后验上我们对0.6这个值的信心更高了。而且随着连续观察的次数\(n\)增加，我们的信息会越来越大，直到最后我们几乎确定&lt;em&gt;太阳升起的概率不是0.5，而是更高的0.6&lt;/em&gt;。这里对于\(q\)分布假设只是个简单的两点分布，而如果我们假设\(q\)的分布更复杂一点，并且其最大可能值0.99分配到了一个很小的非零先验概率0.00001，类似上面的分析，虽然先验很低，但是不要多少天之后屌丝会逆袭，届时我们将笃定太阳明天升起的概率是0.99。&lt;/h3&gt;</content><author><name></name></author><summary type="html">明天太阳还会照常升起吗？</summary></entry><entry><title type="html">深度学习中的数值稳定性问题一例</title><link href="http://localhost:4000/2017/10/29/numerical-stability/" rel="alternate" type="text/html" title="深度学习中的数值稳定性问题一例" /><published>2017-10-29T00:00:00+08:00</published><updated>2017-10-29T00:00:00+08:00</updated><id>http://localhost:4000/2017/10/29/numerical-stability</id><content type="html" xml:base="http://localhost:4000/2017/10/29/numerical-stability/">&lt;h1 id=&quot;深度学习中的数值稳定性问题一例&quot;&gt;深度学习中的数值稳定性问题一例&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StableBCELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
             &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StableBCELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;neg_abs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neg_abs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;上面这段代码中的-forward-函数用来计算所谓的-binary-cross-entropy其实也就是逻辑回归中的损失函数&quot;&gt;上面&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/751&quot;&gt;这段代码&lt;/a&gt;中的 forward 函数用来计算所谓的 binary cross entropy，其实也就是逻辑回归中的损失函数&lt;/h3&gt;
&lt;p&gt;\[ L(y,x) = -y \cdot log(sigmoid(x))-(1-y) \cdot log(1-sigmoid(x))\]&lt;/p&gt;
&lt;h3 id=&quot;其中-sigmoid-函数定义如下&quot;&gt;其中 sigmoid 函数定义如下：&lt;/h3&gt;
&lt;p&gt;\[ sigmoid(x) = \frac {1} {1+e^{-x}} \]&lt;/p&gt;

&lt;h3 id=&quot;既然只是为了计算各交叉熵而已为什么要搞这么复杂的两行呢&quot;&gt;既然只是为了计算各交叉熵而已，为什么要搞这么复杂的两行呢：&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;neg_abs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neg_abs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;原因在于--ex--在-x-是较大正数的时候会出现溢出也就是超出计算机能表达的范围从而造成计算误差而上面这段代码就能在避免计算较大正数的自然指数的同时依然能正确的计算交叉熵可以发现上面代码的指数运算其操作数是非正数&quot;&gt;原因在于 \( e^x \) 在\( x \)是较大正数的时候会出现溢出，也就是超出计算机能表达的范围，从而造成计算误差，而上面这段代码就能在避免计算较大正数的自然指数的同时依然能正确的计算交叉熵。可以发现上面代码的指数运算其操作数是非正数。&lt;/h3&gt;
&lt;h3 id=&quot;下面我们检查下这段代码的正确性代码中的-input-对应-l-定义中的-xtarget-对应-l-定义中的-y&quot;&gt;下面我们检查下这段代码的正确性，代码中的 input 对应 \(L\) 定义中的 \(x\)，target 对应 \(L\) 定义中的 \(y\)。&lt;/h3&gt;
&lt;h3 id=&quot;inputclampmin0-的意思是--max0x下面分四种情况讨论这个方法的正确性&quot;&gt;input.clamp(min=0) 的意思是 \( max(0,x)\)，下面分四种情况讨论这个方法的正确性：&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;\( x \ge 0 \) 且 \( y=1 \)，此时代码计算的是：\( x-x+log(1+e^{-x}) = -log(\frac {1} {1+e^{-x}})\)，显然是正确的。&lt;/li&gt;
  &lt;li&gt;\( x \ge 0 \) 且 \( y=0 \)，此时代码计算的是：\( x-0+log(1+e^{-x}) = log(1+e^x) = -log(1-\frac {1} {1+e^{-x}})\)，也是正确的。&lt;/li&gt;
  &lt;li&gt;\( x \lt 0 \) 且 \( y=1 \)，此时代码计算的是：\( 0-x+log(1+e^{x}) = log(1+e^{-x})\)，是正确的。&lt;/li&gt;
  &lt;li&gt;\( x \lt 0 \) 且 \( y=0 \)，此时代码计算的是：\( 0-0+log(1+e^{x}) = log(1+e^{-x})= -log(1-\frac {1} {1+e^{-x}})\)，同样是正确的。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;在-softmax-的计算中也有指数运算也存在这样的数值稳定性问题&quot;&gt;在 softmax 的计算中也有指数运算，也存在这样的数值稳定性问题：&lt;/h3&gt;

&lt;p&gt;\[ p_k = \frac {e^{x_k}} {\sum e^{x_i}}\]&lt;/p&gt;

&lt;h3 id=&quot;解决思路也是一样的就是避免较大正数的自然指数运算&quot;&gt;解决思路也是一样的，就是避免较大正数的自然指数运算：&lt;/h3&gt;

&lt;p&gt;\[ p_k = \frac {e^{(x_k-max)}} {\sum e^{(x_i-max)}}\]&lt;/p&gt;

&lt;h3 id=&quot;max-表示所有-x_i-的最大数这样的一个变化相当于分子分母同除以一个正数所以计算结果是不变的另外每个-x_i-max-显然都小于0于是就不存在大正数的自然指数运算这样的问题了&quot;&gt;max 表示所有 \(x_i\) 的最大数，这样的一个变化相当于分子分母同除以一个正数，所以计算结果是不变的，另外每个 \(x_i-max\) 显然都小于0，于是就不存在大正数的自然指数运算这样的问题了。&lt;/h3&gt;</content><author><name></name></author><summary type="html">深度学习中的数值稳定性问题一例</summary></entry><entry><title type="html">neural network 损失函数非凸性一例</title><link href="http://localhost:4000/2017/10/16/nn-non-conv-proof/" rel="alternate" type="text/html" title="neural network 损失函数非凸性一例" /><published>2017-10-16T00:00:00+08:00</published><updated>2017-10-16T00:00:00+08:00</updated><id>http://localhost:4000/2017/10/16/nn-non-conv-proof</id><content type="html" xml:base="http://localhost:4000/2017/10/16/nn-non-conv-proof/">&lt;h1 id=&quot;neural-network-损失函数非凸性一例&quot;&gt;neural network 损失函数非凸性一例&lt;/h1&gt;

&lt;h3 id=&quot;微博上爱可可转发了一个-quora-上关于证明神经网络损失函数非凸证明的讨论回答者是鼎鼎大名的-ian-goodfellow我不确定完整理解了-gooodfellow-的意思但是以此为启发结合之前在-cs231n-课程上学到的关于可视化损失函数在低维度上图像的方法我大致形成了证明这个问题的方法&quot;&gt;微博上&lt;a href=&quot;http://weibo.com/p/1005051402400261/home?is_all=1&quot;&gt;@爱可可&lt;/a&gt;转发了一个 quora 上关于证明神经网络损失函数&lt;a href=&quot;https://www.quora.com/How-can-you-prove-that-the-loss-functions-in-Deep-Neural-nets-are-non-convex&quot;&gt;非凸证明的讨论&lt;/a&gt;，回答者是鼎鼎大名的 Ian Goodfellow。我不确定完整理解了 Gooodfellow 的意思，但是以此为启发，结合之前在 cs231n 课程上学到的关于可视化损失函数在低维度上图像的方法，我大致形成了证明这个问题的方法。&lt;/h3&gt;

&lt;h3 id=&quot;先描述一下问题定义--lwxy--为某用于分类任务的神经网络的损失函数其中--w--表示所有的可训练参数-x-y--分布表示训练数据中的特征和-label一般来讲--xy--是固定的于是我们即是要证明--l--相对于--w--有可能是非凸的&quot;&gt;先描述一下问题，定义 \( L(W;X,Y) \) 为某用于分类任务的神经网络的损失函数，其中 \( W \) 表示所有的可训练参数，\( X, Y \) 分布表示训练数据中的特征和 label。一般来讲 \( X,Y \) 是固定的，于是我们即是要证明 \( L \) 相对于 \( W \) 有可能是非凸的。&lt;/h3&gt;

&lt;h3 id=&quot;以下提供一个证明步骤可能并不那么严谨但应该足以说明问题&quot;&gt;以下提供一个证明步骤，可能并不那么严谨，但应该足以说明问题：&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;构造一个神经网络实例，并将其损失函数用代码实现。&lt;/li&gt;
  &lt;li&gt;生成两个随机向量 \( W_0, W_1 \) ，构造高维参数定义域 \( W \) 的一个一维子集 \( W_0+a{\cdot}W_1 \) ，其中 \( a \) 是可变标量。&lt;/li&gt;
  &lt;li&gt;定义函数 \( f(a) = L(W_0+a{\cdot}W_1) \) ，证明一元函数 \( f \) 是非凸的。&lt;/li&gt;
  &lt;li&gt;由 \( f \) 的非凸性推出 \( L \) 的非凸性。&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pylab&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pylab&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# %matplotlib inline&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pylab&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pylab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'figure.figsize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Populating the interactive namespace from numpy and matplotlib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;定义非线性激活函数-sigmoid&quot;&gt;定义非线性激活函数 sigmoid。&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;定义一个单隐层的全连接神经网络我们假定该神经网络用于二分类任务损失函数使用交叉熵&quot;&gt;定义一个单隐层的全连接神经网络。我们假定该神经网络用于二分类任务，损失函数使用交叉熵。&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nn_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;生成一些训练数据也就是上面提到的--xy--&quot;&gt;生成一些训练数据，也就是上面提到的 \( X,Y \) 。&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;生成上面提到的--w_0-w_1--根据提到的神经网络定义参数包含隐层-weight隐层-bias输出层-weight输出层-bias&quot;&gt;生成上面提到的 \( W_0, W_1 \) ，根据提到的神经网络定义，参数包含隐层 weight，隐层 bias，输出层 weight，输出层 bias。&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;base_hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_hb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_lw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;\
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;\
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;\
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dir_hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_hb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_lw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;\
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;\
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;\
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_hb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_lw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_lb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1.4699588455846047
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;到这里我们已经能够定义--f--函数了相对于严格的数学证明以下我采用不那么严谨的可视化方法来进行说明我们画出--f--在一定范围内的图像观察其是否非凸&quot;&gt;到这里我们已经能够定义 \( f \) 函数了，相对于严格的数学证明，以下我采用不那么严谨的可视化方法来进行说明。我们画出 \( f \) 在一定范围内的图像，观察其是否非凸。&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_hw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dir_hw&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_hb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dir_hb&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_lw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dir_lw&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dir_lb&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D at 0x12238cfd0&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://gameofdimension.github.io/images/output_15_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;从上面的图像可以很明显的看出来--f--是非凸的下面就来从--f--的非凸性推导--l--的非凸性&quot;&gt;从上面的图像可以很明显的看出来 \( f \) 是非凸的。下面就来从 \( f \) 的非凸性推导 \( L \) 的非凸性。&lt;/h3&gt;

&lt;p&gt;由 \( f \) 非凸，可知有 \( a_1, a_2, \alpha, \beta \) 满足 \( f(\alpha\cdot{a_1} + \beta\cdot{a_2}) &amp;gt; \alpha\cdot{f(a_1)} + \beta\cdot{f(a_2)} \)，其中 \( \alpha &amp;gt; 0, \beta &amp;gt; 0, \alpha + \beta = 1 \)。&lt;/p&gt;

&lt;p&gt;由 \( f \) 与 \( L \) 的关系，我们有 \( f(\alpha\cdot{a_1} + \beta\cdot{a_2}) = L(W_0+({\alpha\cdot{a_1} + \beta\cdot{a_2}}){\cdot}W_1) &amp;gt; \alpha\cdot{L(W_0+a_1{\cdot}W_1)} + \beta\cdot{L(W_0+a_2{\cdot}W_1)} \)&lt;/p&gt;

&lt;p&gt;而 \( L(W_0+({\alpha\cdot{a_1} + \beta\cdot{a_2}}){\cdot}W_1) = L(\alpha\cdot(W_0+{a_1}\cdot{W_1}) + \beta\cdot(W_0 + {a_2}\cdot{W_1})) \) ，从而 \( L(\alpha\cdot(W_0+{a_1}\cdot{W_1}) + \beta\cdot(W_0 + {a_2}\cdot{W_1})) &amp;gt; \alpha\cdot{L(W_0+a_1{\cdot}W_1)} + \beta\cdot{L(W_0+a_2{\cdot}W_1)} \)  ，而这正好说明了 \( L \) 的非凸性。&lt;/p&gt;

&lt;h3 id=&quot;到此我们证明了存在有些神经网络损失函数是非凸的&quot;&gt;到此我们证明了存在有些神经网络损失函数是非凸的。&lt;/h3&gt;

&lt;p&gt;另外对于由 \( f \) 的非凸性推导 \( L \) 的非凸性，或许我们可以借助一个低维类比来获得一些直观理解。&lt;/p&gt;

&lt;p&gt;我们假定 \( W \) 只有2维，那么整个 \( L \) 的图像就可以在三维坐标中展示出来，其形状我们假设是个可能不那么规则的碗状。上面我们做的事情就是在某个位置，从正上方到正下方垂直向这个碗劈一刀，留下的截面就只是一个类似上面的曲线而已。如果我们在这条曲线上发现了一个非凸的实例，那么我们再把视野放到整个碗状图像，这个非凸实例依然是成立的。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">neural network 损失函数非凸性一例</summary></entry><entry><title type="html">隐马可夫模型与中文分词</title><link href="http://localhost:4000/2017/06/11/hmm-and-chinese-word-segment/" rel="alternate" type="text/html" title="隐马可夫模型与中文分词" /><published>2017-06-11T00:00:00+08:00</published><updated>2017-06-11T00:00:00+08:00</updated><id>http://localhost:4000/2017/06/11/hmm-and-chinese-word-segment</id><content type="html" xml:base="http://localhost:4000/2017/06/11/hmm-and-chinese-word-segment/">&lt;h1 id=&quot;隐马可夫模型与中文分词&quot;&gt;隐马可夫模型与中文分词&lt;/h1&gt;

&lt;h3 id=&quot;前段时间为了建模模板问题研究了一下-hmm-模型虽然最终并没有用-hmm-来解决模板问题但是好歹-hmm-给我们提供了重要的思路在这里把之前了解的关于-hmm-的知识总结记录一下&quot;&gt;前段时间为了建模模板问题，研究了一下 hmm 模型，虽然最终并没有用 hmm 来解决模板问题，但是好歹 hmm 给我们提供了重要的思路。在这里把之前了解的关于 hmm 的知识总结记录一下。&lt;/h3&gt;

&lt;h3 id=&quot;hmm-假定观察到的事实是由不可观察的隐藏变量决定的同时此刻的隐藏变量只取决于上一时刻的隐藏变量状态而与再之前的隐藏变量状态无关同时这种依赖关系也与时间无关也就是说时刻1时隐藏变量对时刻0时隐藏变量状态的依赖关系跟时刻10时隐藏变量对时刻9时隐藏变量的依赖关系完全一样假定时刻-t-时的隐藏状态为--s_t-此刻的可观察变量为--o_t-用数学公式表达以上关系则为&quot;&gt;hmm 假定观察到的事实是由不可观察的隐藏变量决定的，同时此刻的隐藏变量只取决于上一时刻的隐藏变量状态，而与再之前的隐藏变量状态无关。同时这种依赖关系也与时间无关，也就是说时刻1时隐藏变量对时刻0时隐藏变量状态的依赖关系跟时刻10时隐藏变量对时刻9时隐藏变量的依赖关系完全一样。假定时刻 t 时的隐藏状态为 \( s_t \)，此刻的可观察变量为 \( o_t \)。用数学公式表达以上关系则为：&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;\( p(o_t|o_{t-1},o_{t-2}..,s_{t-1},s_{t-2}..) = p(o_t|s_{t-1}) \)     &lt;br /&gt;
\( p(s_t|s_{t-1},s_{t-2}..) = p(s_t|s_{t-1}) \)      &lt;br /&gt;
\( p(S_t|S_{t-1})=constant \)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;要说明的上面第三点并不是说对于某个具体相邻时刻隐藏状态赋值有这种关系而是说只要相邻的两个时刻隐藏状态赋值一样这种概率关系与时间无关而相邻的两个时刻隐藏状态赋值是有许多种可能的&quot;&gt;要说明的上面第三点并不是说对于某个具体相邻时刻隐藏状态赋值有这种关系，而是说只要相邻的两个时刻隐藏状态赋值一样，这种概率关系与时间无关。而相邻的两个时刻隐藏状态赋值是有许多种可能的。&lt;/h3&gt;

&lt;h3 id=&quot;于是从定义上讲一个-hmm-由以下5个元素构成&quot;&gt;于是从定义上讲，一个 hmm 由以下5个元素构成：&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;StatusSet：隐藏状态集合&lt;/li&gt;
  &lt;li&gt;ObservedSet：观察值集合&lt;/li&gt;
  &lt;li&gt;TransProbMatrix：隐藏状态之间的转移概率矩阵&lt;/li&gt;
  &lt;li&gt;EmitProbMatrix：隐藏状态决定观察值的发射概率矩阵&lt;/li&gt;
  &lt;li&gt;InitStatus：初始状态概率分布&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;hmm-模型中通常有三类问题&quot;&gt;hmm 模型中通常有三类问题：&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;已知模型参数和观察序列 \( (o_1,o_2,..,o_T) \) ，求观测序列在给定模型下出现的概率。&lt;/li&gt;
  &lt;li&gt;已知模型和观察序列，求概率最大隐藏状态序列。&lt;/li&gt;
  &lt;li&gt;已知观察序列，估计模型参数。也就是求得模型参数，使得在此模型下，观察序列出现的概率最大。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第一类问题只是简单的计算问题只是其中计算方法需要些技巧一般的方法是动态规划分前向与后向两种思路&quot;&gt;第一类问题只是简单的计算问题，只是其中计算方法需要些技巧，一般的方法是动态规划，分前向与后向两种思路。&lt;/h3&gt;

&lt;h3 id=&quot;第三类问题因为输入信息量少但是输出信息量却很大其效果也要打问号一般的求解方法是-em-算法&quot;&gt;第三类问题因为输入信息量少，但是输出信息量却很大，其效果也要打问号。一般的求解方法是 EM 算法。&lt;/h3&gt;

&lt;h3 id=&quot;第二类问题应用广泛比如著名的中文分词问题就可以用其建模接下来就详细说明一下&quot;&gt;第二类问题应用广泛，比如著名的中文分词问题就可以用其建模。接下来就详细说明一下。&lt;/h3&gt;

&lt;h3 id=&quot;假定有一个中文句子隐藏状态之间的转移概率矩阵这里的每个汉字就对应一个观察值而观察值背后的隐藏状态则如下定义&quot;&gt;假定有一个中文句子：“隐藏状态之间的转移概率矩阵”。这里的每个汉字就对应一个观察值，而观察值背后的隐藏状态则如下定义：&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;B：词开始
M：词中部
E：词结尾
S：单字成词&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;比如隐字对应的隐藏状态就是-b--藏字对应对应的隐藏状态就是-e--的-字对应的隐藏状态就是-s-对于一句话我们只要确定了每个字对应的隐藏状态也就确定了这句话的分词方案&quot;&gt;比如“隐”字对应的隐藏状态就是 B , “藏”字对应对应的隐藏状态就是 E , “的” 字对应的隐藏状态就是 S 。对于一句话我们只要确定了每个字对应的隐藏状态，也就确定了这句话的分词方案。&lt;/h3&gt;

&lt;h3 id=&quot;对比上面的模型5要素我们已经有了-statusset-而另外四个部分都可以通过统计语料库获得&quot;&gt;对比上面的模型5要素，我们已经有了 StatusSet， 而另外四个部分都可以通过统计语料库获得。&lt;/h3&gt;

&lt;h3 id=&quot;对于我们要解决的问题当然有一个-naive-的算法就是枚举所有可能的隐藏状态序列计算其概率选择对应概率最大的那个序列赋值这个算法虽然直观但是确是低效的其时间效率是指数级别的&quot;&gt;对于我们要解决的问题，当然有一个 naive 的算法，就是枚举所有可能的隐藏状态序列，计算其概率，选择对应概率最大的那个序列赋值。这个算法虽然直观，但是确是低效的，其时间效率是指数级别的。&lt;/h3&gt;

&lt;h3 id=&quot;通常解决这个问题的算法是维特比算法这个算法其实也就是动态规划方法的具体应用如下面这张图就是要求从起点-start-开始经过中间每一层到达最后一层的一个最大路径要注意的是这个图是有向的而且只在相邻层之间有边连接将问题做了这样的抽象之后要得出详细的递推关系也就不难了具体可以参照维基百科&quot;&gt;通常解决这个问题的算法是维特比算法，这个算法其实也就是动态规划方法的具体应用。如下面这张图，就是要求从起点 start 开始，经过中间每一层到达最后一层的一个最大路径。要注意的是这个图是有向的，而且只在相邻层之间有边连接。将问题做了这样的抽象之后，要得出详细的递推关系也就不难了，具体可以参照维基百科。&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://gameofdimension.github.io/images/14971947528379.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考：&lt;/h2&gt;
&lt;p&gt;统计学习方法，李航
&lt;a href=&quot;http://yanyiwu.com/work/2014/04/07/hmm-segment-xiangjie.html&quot;&gt;中文分词之HMM模型详解&lt;/a&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;维基百科&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">隐马可夫模型与中文分词</summary></entry></feed>